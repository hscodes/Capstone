{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Loading libraries\n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from sklearn import feature_extraction\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# NLTK\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('./preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>stance_dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>police find mass graf least  body near mexico ...</td>\n",
       "      <td>712</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>danny boyle directing untitled film seth rogen...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>seth rogen play apple steve wozniak</td>\n",
       "      <td>712</td>\n",
       "      <td>discuss</td>\n",
       "      <td>danny boyle directing untitled film seth rogen...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>mexico police find mass grave near site  stude...</td>\n",
       "      <td>712</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>danny boyle directing untitled film seth rogen...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>mexico say missing student found first mass graf</td>\n",
       "      <td>712</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>danny boyle directing untitled film seth rogen...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>new io  bug delete icloud document</td>\n",
       "      <td>712</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>danny boyle directing untitled film seth rogen...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "3           3             3   \n",
       "4           4             4   \n",
       "\n",
       "                                            Headline  Body ID     Stance  \\\n",
       "0  police find mass graf least  body near mexico ...      712  unrelated   \n",
       "1                seth rogen play apple steve wozniak      712    discuss   \n",
       "2  mexico police find mass grave near site  stude...      712  unrelated   \n",
       "3   mexico say missing student found first mass graf      712  unrelated   \n",
       "4                 new io  bug delete icloud document      712  unrelated   \n",
       "\n",
       "                                         articleBody  stance_dummy  \n",
       "0  danny boyle directing untitled film seth rogen...             3  \n",
       "1  danny boyle directing untitled film seth rogen...             2  \n",
       "2  danny boyle directing untitled film seth rogen...             3  \n",
       "3  danny boyle directing untitled film seth rogen...             3  \n",
       "4  danny boyle directing untitled film seth rogen...             3  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agree</th>\n",
       "      <th>disagree</th>\n",
       "      <th>discuss</th>\n",
       "      <th>unrelated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agree  disagree  discuss  unrelated\n",
       "0      0         0        0          1\n",
       "1      0         0        1          0\n",
       "2      0         0        0          1\n",
       "3      0         0        0          1\n",
       "4      0         0        0          1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stance_dummy = pd.get_dummies(df_all['Stance'], drop_first=False)\n",
    "stance_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "Using word embeddings on both the headline and article bodies. This pretrained model is called the Global Vectors for Word Representation or GloVE. It is pretrained on 6 billion words and can use its 400,000 word vocabulary for vectors of 50, 100, 200, or 300.\n",
    "I use a keras preprocessor to turn words in to integer sequence which creates an index of all the words, selects the top most used words to track, then turns corpus into list of word sequences represented by integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2903 unique tokens/words\n",
      "The maximum word index is 2903\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "max_sequence_length = 100   # max number of words to consider in a review (i.e. the first 50...)\n",
    "max_num_words = 4000       # maximum number of words to include in the vocabulary (i.e. top 10000 only)\n",
    "\n",
    "# Instantiate the Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_num_words)\n",
    "\n",
    "# Fit the tokenizer, i.e. learn the vocab and id the most frequently occuring words\n",
    "tokenizer.fit_on_texts(df_all['Headline'].values)\n",
    "\n",
    "# Turn our texts to sequences of word indices\n",
    "sequences = tokenizer.texts_to_sequences(df_all['Headline'].values)\n",
    "\n",
    "# Save the look-up dictionary for words to indices (will need this later)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Pad out our sequences by prepending zeros to all of our text sequences\n",
    "padded_sequences_headline = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "print(f'Found {len(word_index)} unique tokens/words')\n",
    "print(f'The maximum word index is {padded_sequences_headline.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49972, 100)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences_headline.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  208,\n",
       "        251,  102,  311,  138,  245,  165,   80,  878,   59, 1942,  208,\n",
       "       1943], dtype=int32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences_headline[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19876 unique tokens/words\n",
      "The maximum word index is 19876\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 200   # max number of words to consider in a review (i.e. the first 50...)\n",
    "max_num_words = 25000     # maximum number of words to include in the vocabulary (i.e. top 10000 only)\n",
    "\n",
    "# Instantiate the Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_num_words)\n",
    "\n",
    "# Fit the tokenizer, i.e. learn the vocab and id the most frequently occuring words\n",
    "tokenizer.fit_on_texts(df_all['articleBody'].values)\n",
    "\n",
    "# Turn our texts to sequences of word indices\n",
    "sequences = tokenizer.texts_to_sequences(df_all['articleBody'].values)\n",
    "\n",
    "# Save the look-up dictionary for words to indices (will need this later)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Pad out our sequences by prepending zeros to all of our text sequences\n",
    "padded_sequences_articleBody = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "print(f'Found {len(word_index)} unique tokens/words')\n",
    "print(f'The maximum word index is {padded_sequences_articleBody.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49972, 200)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences_articleBody.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  278,    8, 1073],\n",
       "       [   0,    0,    0, ...,  278,    8, 1073],\n",
       "       [   0,    0,    0, ...,  278,    8, 1073],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  159,  245, 2899],\n",
       "       [3013, 1672,  518, ...,  157,  187,  960],\n",
       "       [   0,    0,    0, ...,  292,  266,  733]], dtype=int32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences_articleBody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the padded sequences of both headline and article body together before running model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49972, 300)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences = np.concatenate((padded_sequences_headline, padded_sequences_articleBody), axis=1)\n",
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49972,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['stance_dummy'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33481, 300), (33481, 4))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train, test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, stance_dummy, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors\n"
     ]
    }
   ],
   "source": [
    "#reading in GloVe\n",
    "import numpy as np\n",
    "\n",
    "glove_dir = '../glove.6B (1)/'\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print(f'Found {len(embeddings_index)} word vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create matrix for word vectors for words in tokenized vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_num_words, embedding_dim))\n",
    "for word, index in word_index.items():\n",
    "    if index < max_num_words:\n",
    "        # Using \"get\" with the dict is a safe way to avoid missing key errors\n",
    "        embedding_vector = embeddings_index.get(word) \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_num_words, embedding_dim, input_length=300))\n",
    "model.add(LSTM(20))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pretrained GloVe weights in matrix(embedding matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding layer is input for LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 300, 100)          2500000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 20)                9680      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 2,509,764\n",
      "Trainable params: 2,509,764\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33481 samples, validate on 16491 samples\n",
      "Epoch 1/10\n",
      "33481/33481 [==============================] - 100s 3ms/step - loss: 0.3712 - acc: 0.8481 - val_loss: 0.3268 - val_acc: 0.8662\n",
      "Epoch 2/10\n",
      "33481/33481 [==============================] - 90s 3ms/step - loss: 0.2931 - acc: 0.8840 - val_loss: 0.2863 - val_acc: 0.8891\n",
      "Epoch 3/10\n",
      "33481/33481 [==============================] - 81s 2ms/step - loss: 0.2618 - acc: 0.8991 - val_loss: 0.2712 - val_acc: 0.8950\n",
      "Epoch 4/10\n",
      "33481/33481 [==============================] - 80s 2ms/step - loss: 0.2474 - acc: 0.9045 - val_loss: 0.2628 - val_acc: 0.9003\n",
      "Epoch 5/10\n",
      "33481/33481 [==============================] - 81s 2ms/step - loss: 0.2373 - acc: 0.9074 - val_loss: 0.2568 - val_acc: 0.9027\n",
      "Epoch 6/10\n",
      "33481/33481 [==============================] - 89s 3ms/step - loss: 0.2306 - acc: 0.9096 - val_loss: 0.2522 - val_acc: 0.9026\n",
      "Epoch 7/10\n",
      "33481/33481 [==============================] - 90s 3ms/step - loss: 0.2256 - acc: 0.9106 - val_loss: 0.2500 - val_acc: 0.9041\n",
      "Epoch 8/10\n",
      "33481/33481 [==============================] - 79s 2ms/step - loss: 0.2221 - acc: 0.9113 - val_loss: 0.2475 - val_acc: 0.9028\n",
      "Epoch 9/10\n",
      "33481/33481 [==============================] - 91s 3ms/step - loss: 0.2189 - acc: 0.9122 - val_loss: 0.2457 - val_acc: 0.9039\n",
      "Epoch 10/10\n",
      "33481/33481 [==============================] - 76s 2ms/step - loss: 0.2176 - acc: 0.9123 - val_loss: 0.2452 - val_acc: 0.9026\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=300, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 1.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's baseline?\n",
    "np.unique(y_train, return_counts=True)[1]/len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For prediction, passing neural net a sequence of word indices with sequence length equal to max sequence length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0:1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01394238, 0.0048189 , 0.00776374, 0.973475  ]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[0:1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary \n",
    "\n",
    "My goal was to classify my data(headlines and articles) by stance detection. Stance detection classifies the stance of the body text relative to the claim made in the headline into four categories: agree, disagree, discuss, and unrelated. Stance detection is currently used to verify what other news organizations are saying about a topic and is the first step in automating detection of fake news. The version of stance detection I will be using is building on the work of William Ferreira and Andreas Vlachos. I recieved my data from the data provided by the FakeNewsChallenge.org. The data is taken from the Emergent Dataset created by Craig Silverman, an expert in \"Fake News\". Emergent is a \"real time rumor tracker\" focusing on how rumors are reported in the media. \n",
    "\n",
    "My model begins with the baseline class at 73%. Through implementation of count vectorizer, TF-IDF, and using Logistic Regression and Random Trees to classify my metrics moved little past 73%. While it seemed that no form of vectorization was helping increase the score, the ROC model showed the micro average at 88% giving me hope that a higher score could be reached in some way. That is when I learned about word2vec and using LSTMs for a neural network. After some failures with word2vec, I implemented the GloVe model which is far superior to word2vec because it has been pretrained on many more words than just my dataset. Using word embeddings, I was able to train a neural network that scored 90% accuracy. \n",
    "\n",
    "My findings were that word embeddings and neural networks were necessary to create a model above baseline to predict stance. Dummying the stance label also helped tighten the results. If given more time, I would have run more classification models and tried to train my own neural network with more data. I would have also liked to have predicted more specifically for the desired classes. While my score currently exceeds what is on Kaggle, it is because I was not predicting for the agree and disagree and rather all the stances. I would have also have liked to play with LDA similarities some more to see what could be done with those. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
